{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a556b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whisper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whisper'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import whisper\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a68383",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILE = \"audio.wav\"\n",
    "VIDEO_FILE = \"video.mp4\"\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdf22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio & extract spectrogram\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_FILE)\n",
    "print(f\"Audio: {waveform.shape}, Sample: {sample_rate}\")\n",
    "\n",
    "# Compute spectrogram\n",
    "spectro = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
    "spec_db = torchaudio.transforms.AmplitudeToDB()(spectro)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(spec_db[0].numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Extract MFCCs\n",
    "mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=13)(waveform)\n",
    "mfcc_features = mfcc.mean(dim=2).squeeze().numpy()\n",
    "print(\"MFCC:\", mfcc_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use whisper to transform audio \n",
    "model = whisper.load_model(\"small\")  # can use \"tiny\" if GPU is limited\n",
    "result = model.transcribe(AUDIO_FILE)\n",
    "transcript = result[\"text\"]\n",
    "print(\"Transcript:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use file from OpenCV\n",
    "cap = cv2.VideoCapture(VIDEO_FILE)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Video frames: {frame_count}, FPS: {fps}\")\n",
    "\n",
    "frame_paths = []\n",
    "for i in range(0, frame_count, int(fps*2)):  # pause, 2 sec each time \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame_path = OUTPUT_DIR / f\"frame_{i}.jpg\"\n",
    "        cv2.imwrite(str(frame_path), frame)\n",
    "        frame_paths.append(frame_path)\n",
    "cap.release()\n",
    "\n",
    "print(\"Extracted frames:\", frame_paths[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f04d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine \n",
    "data = {\n",
    "    \"transcript\": [transcript],\n",
    "    \"mfcc_features\": [mfcc_features.tolist()],\n",
    "    \"frame_paths\": [list(map(str, frame_paths))]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(OUTPUT_DIR / \"multimodal_dataset.csv\", index=False)\n",
    "df # Show what it looks like now to have a brief idea "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lseg)",
   "language": "python",
   "name": "lseg-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
